{"componentChunkName":"component---src-templates-post-template-post-template-tsx","path":"/machine-learning/linear-regression-with-multiple-variables","result":{"data":{"markdownRemark":{"id":"e3b3cb94-8a0f-5cbb-8e75-f4a12e4988a4","html":"<h1 id=\"machine-learning---linear-regression-with-multiple-variables\" style=\"position:relative;\"><a href=\"#machine-learning---linear-regression-with-multiple-variables\" aria-label=\"machine learning   linear regression with multiple variables permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Machine Learning - Linear Regression with Multiple Variables</h1>\n<h2 id=\"multiple-features\" style=\"position:relative;\"><a href=\"#multiple-features\" aria-label=\"multiple features permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Multiple Features</h2>\n<ol>\n<li>\n<p>When more than one feature affects the output</p>\n</li>\n<li>\n<p>In the example of estimating house price, size, # of bedrooms, # of floors may affect the price</p>\n</li>\n<li>\n<p>features are denoted with x<sub>1</sub>, x<sub>2</sub></p>\n<ul>\n<li>n = # of features</li>\n<li>x<sup>(i)</sup> = input of i<sup>th</sup> training example : vector</li>\n<li>x<sub>j</sub><sup>(i)</sup> = value of feature j in i<sup>th</sup> training example : the value</li>\n</ul>\n</li>\n<li>\n<p>Since there are more than one features, the hypothesis is going to get longer in order to include more terms/features</p>\n<ul>\n<li>theta represents parameter: I think this is like weight</li>\n<li>x<sub>0</sub> is assumed to be 1</li>\n<li>x and theta are both (n+1) dimensional vectors</li>\n<li>h<sub>θ</sub>(x) = θ<sup>T</sup>x</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"gradient-descent-for-multiple-variables\" style=\"position:relative;\"><a href=\"#gradient-descent-for-multiple-variables\" aria-label=\"gradient descent for multiple variables permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Gradient Descent for Multiple Variables</h2>\n<ul>\n<li>Pretty much the same as single variable, but you gotta multiply x<sub>j</sub><sup>(i)</sup> for each theta</li>\n</ul>\n<h2 id=\"feature-scaling\" style=\"position:relative;\"><a href=\"#feature-scaling\" aria-label=\"feature scaling permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Feature Scaling</h2>\n<ol>\n<li>\n<p>Making features on a similar scale</p>\n<ul>\n<li>if the ranges are different between features, the contour plot could be skewed</li>\n<li>divide the features by the largest possible value to make the contour plot to look like a circle as much as possible</li>\n<li>because all the features lay within -1 to 1 range</li>\n</ul>\n</li>\n<li>\n<p>Mean Normalization</p>\n<ul>\n<li>subtract the feature by the mean of the values and divide that by the largest value</li>\n<li>x<sub>i</sub> = (x<sub>i</sub> - μ<sub>i</sub>) / largest value</li>\n<li>you could also divide the numerator by the difference between the smallest and the largest</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"learning-rate\" style=\"position:relative;\"><a href=\"#learning-rate\" aria-label=\"learning rate permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Learning Rate</h2>\n<ol>\n<li>\n<p>Tips to ensure learning rate is working correctly</p>\n<ul>\n<li>make sure that the cost function is decreasing as you iterate</li>\n<li>J(θ) must decrease after every iteration</li>\n<li>declare convergence if J(θ) decreases by less than 10<sup>-3</sup> in one iteration</li>\n</ul>\n</li>\n<li>\n<p>Use smaller learning rate if not converging</p>\n<ul>\n<li>a too large learning rate may overshoot and it may diverge instead of converging</li>\n<li>a sufficiently small learning rate is the best</li>\n<li>but if it is too small, it will take too long to converge</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"features-and-polynomial-regression\" style=\"position:relative;\"><a href=\"#features-and-polynomial-regression\" aria-label=\"features and polynomial regression permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Features and Polynomial Regression</h2>\n<ol>\n<li>combine two features like frontage and depth and multiply them to be an area\n<ul>\n<li>define a new feature by combining them to reduce the number of parameters/features</li>\n<li>your cost function may become simpler</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"normal-equation\" style=\"position:relative;\"><a href=\"#normal-equation\" aria-label=\"normal equation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Normal Equation</h2>\n<ol>\n<li>\n<p>Method to solve for θ analytically instead of doing iterations</p>\n<ul>\n<li>polynomial of θ, and take a derivative with respect to θ</li>\n<li>take partial derivatives with respect to θ and set them to zero to find the θ<sub>i</sub></li>\n</ul>\n</li>\n<li>\n<p>θ = (X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>y</p>\n</li>\n<li>\n<p>Slow if n is really large becuase you need to take inverse of the matrix X</p>\n</li>\n<li>\n<p>won’t work if (X<sup>T</sup>X) is not inversible</p>\n<ul>\n<li>you have to delete some features or use regularization</li>\n</ul>\n</li>\n</ol>","fields":{"slug":"/posts/2020//machine-learning/linear-regression-with-multiple-variables","tagSlugs":["/tag/machine-learning/"]},"frontmatter":{"date":"2020-10-29T21:53:37.121Z","description":"Coursera Machine Learning course: Linear Regression with Multiple Variables","tags":["Machine Learning"],"title":"Machine Learning - Linear Regression with Multiple Variables","socialImage":null}}},"pageContext":{"slug":"/posts/2020//machine-learning/linear-regression-with-multiple-variables"}},"staticQueryHashes":["251939775","288581551","401334301"]}