{"componentChunkName":"component---src-templates-post-template-post-template-tsx","path":"/machine-learning/logistic-regression","result":{"data":{"markdownRemark":{"id":"15c5ec0a-56b9-5197-b500-1dab410d787c","html":"<h1 id=\"logistic-regression\" style=\"position:relative;\"><a href=\"#logistic-regression\" aria-label=\"logistic regression permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Logistic Regression</h1>\n<h2 id=\"classification-and-representation\" style=\"position:relative;\"><a href=\"#classification-and-representation\" aria-label=\"classification and representation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Classification and Representation</h2>\n<h3 id=\"classification\" style=\"position:relative;\"><a href=\"#classification\" aria-label=\"classification permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Classification</h3>\n<ol>\n<li>\n<p>yes or no question where outputs are discrete</p>\n<ul>\n<li>0: negative class (benign tumor)</li>\n<li>1:positive class (malignant tumor)</li>\n<li>there could be multi-class classifiction where there are more than two possible outputs</li>\n</ul>\n</li>\n<li>\n<p>You could use this: h<sub>θ</sub>(x) = θ<sup>T</sup>x</p>\n<ul>\n<li>if h<sub>θ</sub>(x) = θ<sup>T</sup>x > 0.5, y = 1 (0.5 is the threshold)</li>\n<li>if h<sub>θ</sub>(x) = θ<sup>T</sup>x &#x3C; 0.5, y = 0</li>\n<li>but what happens if the input range increases? : if the threshold remains the same, some cases that could be benign are now considered to be malignant</li>\n<li>but in some cases, y could be greater than 1 or smaller than 0</li>\n<li>logistic regression ensures that 0 &#x3C; h<sub>θ</sub>(x) &#x3C; 1</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"hypothesis-representation\" style=\"position:relative;\"><a href=\"#hypothesis-representation\" aria-label=\"hypothesis representation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Hypothesis Representation</h3>\n<ol>\n<li>\n<p>a function that represents hypothesis that satifies 0 &#x3C; h<sub>θ</sub>(x) &#x3C; 1</p>\n</li>\n<li>\n<p>h<sub>θ</sub>(x) =g(θ<sup>T</sup>x) where g(z) = 1 / (1 + e<sup>-z</sup>)</p>\n</li>\n<li>\n<p>h<sub>θ</sub>(x) = 1 / (1 + -e<sup>θ<sup>T</sup>x</sup>)</p>\n<ul>\n<li>sigmoid function / logistic function</li>\n<li>asymptote at 0 and 1</li>\n<li>ensures that 0 &#x3C; h<sub>θ</sub>(x) &#x3C; 1</li>\n</ul>\n</li>\n<li>\n<p>h<sub>θ</sub>(x) = estimated probability that y = 1</p>\n<ul>\n<li>h<sub>θ</sub>(x) = P(y=1 | x;θ)</li>\n<li>P(y=0 | x;θ) + P(y=1 | x;θ) = 1</li>\n<li>P(y=0 | x;θ) = 1 - P(y=1 | x;θ)</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"decision-boundary\" style=\"position:relative;\"><a href=\"#decision-boundary\" aria-label=\"decision boundary permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Decision Boundary</h3>\n<ol>\n<li>\n<p>g(z) > 0.5 when z > 0</p>\n<ul>\n<li>h<sub>θ</sub>(x) =g(θ<sup>T</sup>x) > 0.5 when θ<sup>T</sup>x > 0 where θ<sup>T</sup>x = z</li>\n</ul>\n</li>\n<li>\n<p>h<sub>θ</sub>(x) = g(θ<sub>0</sub> + θ<sub>1</sub>x<sub>1</sub> + θ<sub>2</sub>x<sub>2</sub>)</p>\n</li>\n<li>\n<p>non-linear decision boundaries</p>\n<ul>\n<li>sometimes your equation may not be linear</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"logistic-regression-model\" style=\"position:relative;\"><a href=\"#logistic-regression-model\" aria-label=\"logistic regression model permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Logistic Regression Model</h2>\n<h3 id=\"cost-function\" style=\"position:relative;\"><a href=\"#cost-function\" aria-label=\"cost function permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Cost Function</h3>\n<ol>\n<li>\n<p>Linear Regression:</p>\n<ul>\n<li>J(θ) = Cost(h<sub>θ</sub>x, y) =(1/2)(h<sub>θ</sub>x - y)<sup>2</sup></li>\n</ul>\n</li>\n<li>\n<p>For Logistic Regression:</p>\n<ul>\n<li>the cost function ends up non-convex if square is used</li>\n<li>many local optima may appear</li>\n<li>log(h<sub>θ</sub>x) if y = 1</li>\n<li>-log(1-h<sub>θ</sub>x) if y = 0</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"simplified-cost-function-and-gradient-descent\" style=\"position:relative;\"><a href=\"#simplified-cost-function-and-gradient-descent\" aria-label=\"simplified cost function and gradient descent permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Simplified Cost Function and Gradient Descent</h3>\n<ol>\n<li>J(θ) = Cost(h<sub>θ</sub>x, y) = -y*log(h<sub>θ</sub>x) -(1-y)log(1-h<sub>θ</sub>x)</li>\n</ol>\n<h3 id=\"advanced-optimization\" style=\"position:relative;\"><a href=\"#advanced-optimization\" aria-label=\"advanced optimization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Advanced Optimization</h3>\n<ul>\n<li>Conjugate gradient, BFGS, L-BFGS\n<ul>\n<li>no need to manually pick a learning rate</li>\n<li>often faster than gradient descent, but more complex</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"regularization\" style=\"position:relative;\"><a href=\"#regularization\" aria-label=\"regularization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Regularization</h2>\n<h3 id=\"problem-of-overfitting\" style=\"position:relative;\"><a href=\"#problem-of-overfitting\" aria-label=\"problem of overfitting permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Problem of Overfitting</h3>\n<ul>\n<li>trying your best to fit the training set</li>\n<li>could be like 5 orders</li>\n</ul>\n<ol>\n<li>\n<p>underfitting</p>\n<ul>\n<li>does not fit the training set very well</li>\n<li>also called high bias</li>\n</ul>\n</li>\n<li>\n<p>overfitting</p>\n<ul>\n<li>graph looks weird to best fit the data</li>\n<li>also called high variance</li>\n</ul>\n</li>\n<li>\n<p>How to solve the overfitting problem</p>\n<ul>\n<li>reduce number of features</li>\n<li>model selection algorithm</li>\n<li>regularization: keep all the features but reduce the magnitudes of parameters</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"cost-function-1\" style=\"position:relative;\"><a href=\"#cost-function-1\" aria-label=\"cost function 1 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Cost Function</h3>\n<ol>\n<li>\n<p>Small values for parameters</p>\n<ul>\n<li>simpler hypothesis</li>\n<li>less prone to overfitting</li>\n<li>add a lambda to deal with parameters</li>\n</ul>\n</li>\n<li>\n<p>Regularization term which includes lambda</p>\n<ul>\n<li>too large of a lambda results in underfitting</li>\n</ul>\n</li>\n</ol>","fields":{"slug":"/posts/2020//machine-learning/logistic-regression","tagSlugs":["/tag/machine-learning/"]},"frontmatter":{"date":"2020-11-03T00:34:37.121Z","description":"Coursera Machine Learning course: Logistic Regression","tags":["Machine Learning"],"title":"Machine Learning - Logistic Regression","socialImage":null}}},"pageContext":{"slug":"/posts/2020//machine-learning/logistic-regression"}},"staticQueryHashes":["251939775","288581551","401334301"]}