{"componentChunkName":"component---src-templates-post-template-post-template-tsx","path":"/machine-learning/neural-networks-representation","result":{"data":{"markdownRemark":{"id":"01c4a725-fa8c-5571-b799-7983c701d4c6","html":"<h1 id=\"neural-networks\" style=\"position:relative;\"><a href=\"#neural-networks\" aria-label=\"neural networks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Neural Networks</h1>\n<h2 id=\"model-representation\" style=\"position:relative;\"><a href=\"#model-representation\" aria-label=\"model representation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Model Representation</h2>\n<ol>\n<li>\n<p>input and output</p>\n<ul>\n<li>x<sub>1</sub>, x<sub>2</sub> are input layers which go through a node and then generate an output layer</li>\n<li>h<sub>θ</sub>(x) = 1 / (1 + -e<sup>θ<sup>T</sup>x</sup>)</li>\n<li>x<sub>0</sub> is usually 1, which is also called a bias unit</li>\n</ul>\n</li>\n<li>\n<p>sigmoid activation function</p>\n<ul>\n<li>g(z) = 1 / (1 + e<sup>-z</sup>)</li>\n</ul>\n</li>\n<li>\n<p>Neural network is a group of neurons put together</p>\n<ul>\n<li>hidden layers are between the input layer and the output layer</li>\n<li>weights are also matrix</li>\n<li>s<sub>j</sub> units in layer j, s<sub>j+1</sub> units in layer j+1</li>\n<li>the dimension will be s<sub>j+1</sub> * (s<sub>j</sub> + 1)</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"intuition\" style=\"position:relative;\"><a href=\"#intuition\" aria-label=\"intuition permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Intuition</h2>\n<ol>\n<li>Non-linear classification\n<ul>\n<li>if data can be clusterd, try to use a simple representation of a given data set</li>\n<li>x<sub>1</sub> XOR x<sub>2</sub>: true if either one is true</li>\n<li>x<sub>1</sub> XNOR x<sub>2</sub> : NOT (x<sub>1</sub> XOR x<sub>2</sub>)</li>\n<li>x<sub>1</sub> AND x<sub>2</sub></li>\n<li>do some calculation and see if the function becomes 1 with x<sub>1</sub> and x<sub>2</sub> are either 0 or 1</li>\n</ul>\n</li>\n</ol>","fields":{"slug":"/posts/2020//machine-learning/neural-networks-representation","tagSlugs":["/tag/machine-learning/"]},"frontmatter":{"date":"2020-11-04T23:34:37.121Z","description":"Coursera Machine Learning course: Neural Networks - Representation","tags":["Machine Learning"],"title":"Machine Learning - Neural Networks - Representation","socialImage":null}}},"pageContext":{"slug":"/posts/2020//machine-learning/neural-networks-representation"}},"staticQueryHashes":["251939775","288581551","401334301"]}