{"componentChunkName":"component---src-templates-tag-template-tag-template-tsx","path":"/tag/llm","result":{"data":{"site":{"siteMetadata":{"title":"Blog by Jason Kang","subtitle":"AI Enthusiast and a Software Engineer"}},"allMarkdownRemark":{"edges":[{"node":{"fields":{"slug":"/posts/2025//llm/building-an-sat-reading-prep-application","categorySlug":"/category/llm/"},"frontmatter":{"title":"Building an SAT Reading Prep Application","date":"2025-01-28T20:35:37.121Z","category":"LLM","description":"Explore the development of an SAT Reading Prep Application using gpt-4o and Cursor","slug":"/llm/building-an-sat-reading-prep-application"}}},{"node":{"fields":{"slug":"/posts/2025//llm/how-does-huggingface-transformers-work","categorySlug":"/category/llm/"},"frontmatter":{"title":"How does Huggingface Transformers work?","date":"2025-01-18T20:35:37.121Z","category":"LLM","description":"A deep dive into how Huggingface Transformers works under the hood, exploring its pipeline architecture, model loading process, and key functionalities that make it a powerful tool for working with transformer models.","slug":"/llm/how-does-huggingface-transformers-work"}}},{"node":{"fields":{"slug":"/posts/2024//llm/how-to-use-llm-as-a-reranker","categorySlug":"/category/llm/"},"frontmatter":{"title":"Using LLM as a Reranker","date":"2024-12-19T20:35:37.121Z","category":"LLM","description":"Explore the concept of Rerankers, their role in enhancing search results, and how they leverage large language models to improve the relevance and accuracy of information retrieval.","slug":"/llm/how-to-use-llm-as-a-reranker"}}},{"node":{"fields":{"slug":"/posts/2024//llm/how-is-attention-score-calculated","categorySlug":"/category/llm/"},"frontmatter":{"title":"How Is Attention Calculated?","date":"2024-12-01T20:35:37.121Z","category":"LLM","description":"A detailed exploration of how attentions are calculated in the Transformer model, as introduced in 'Attention Is All You Need.'","slug":"/llm/how-is-attention-score-calculated"}}}]}},"pageContext":{"group":"LLM","limit":4,"offset":0,"pagination":{"currentPage":0,"prevPagePath":"/tag/llm","nextPagePath":"/tag/llm/page/1","hasNextPage":true,"hasPrevPage":false}}},"staticQueryHashes":["251939775","288581551","401334301","63107425"]}