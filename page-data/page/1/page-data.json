{"componentChunkName":"component---src-templates-index-template-index-template-tsx","path":"/page/1","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"fields":{"categorySlug":"/category/llm/","slug":"/posts/2024//llm/how-to-use-llm-as-a-reranker"},"frontmatter":{"description":"Explore the concept of Rerankers, their role in enhancing search results, and how they leverage large language models to improve the relevance and accuracy of information retrieval.","category":"LLM","title":"Using LLM as a Reranker","date":"2024-12-19T20:35:37.121Z","slug":"/llm/how-to-use-llm-as-a-reranker"}}},{"node":{"fields":{"categorySlug":"/category/llm/","slug":"/posts/2024//llm/how-is-attention-score-calculated"},"frontmatter":{"description":"A detailed exploration of how attentions are calculated in the Transformer model, as introduced in 'Attention Is All You Need.'","category":"LLM","title":"How Is Attention Calculated?","date":"2024-12-01T20:35:37.121Z","slug":"/llm/how-is-attention-score-calculated"}}},{"node":{"fields":{"categorySlug":"/category/llm/","slug":"/posts/2024//llm/what-is-attention"},"frontmatter":{"description":"An exploration of the concept of Attention in LLMs, discussing its significance and impact on model performance and understanding.","category":"LLM","title":"What Is Attention?","date":"2024-11-30T20:35:37.121Z","slug":"/llm/what-is-attention"}}},{"node":{"fields":{"categorySlug":"/category/llm/","slug":"/posts/2024//llm/why-evaluation-is-important"},"frontmatter":{"description":"Exploring the significance of evaluation in developing LLM applications","category":"LLM","title":"Why Is Evaluation Important in Building an LLM Application?","date":"2024-11-21T20:35:37.121Z","slug":"/llm/why-evaluation-is-important"}}}]}},"pageContext":{"limit":4,"offset":4,"pagination":{"currentPage":1,"prevPagePath":"/","nextPagePath":"/page/2","hasNextPage":true,"hasPrevPage":true}}},"staticQueryHashes":["251939775","288581551","401334301","63107425"]}