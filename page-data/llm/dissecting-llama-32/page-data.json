{"componentChunkName":"component---src-templates-post-template-post-template-tsx","path":"/llm/dissecting-llama-32","result":{"data":{"markdownRemark":{"id":"eec983aa-ba50-597c-8b73-2aa175d69c23","html":"<p>Llama3.2 was released last month, featuring medium-sized vision LLMs (11B and 90B) alongside lightweight, text-only models (1B and 3B). Earlier this year, I worked on a mobile application designed to assist users in environments such as refineries and power plants—critical infrastructure where safety regulations and data protection are paramount. Given the often unstable internet connections at these sites, I believed it would be advantageous to explore one of the lightweight text-only models for both performance and security reasons. Consequently, I reviewed the <a href=\"https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Llama3.2 release notes</a> to gain insights into its development.\n<br><br></p>\n<h1 id=\"vision-models\" style=\"position:relative;\"><a href=\"#vision-models\" aria-label=\"vision models permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Vision models</h1>\n<ul>\n<li>Llama3.2 models (11B and 90B parameters) are the first in the Llama series to support vision tasks.</li>\n<li>Requires a new architecture for image reasoning to integrate visual input with existing language capabilities.</li>\n</ul>\n<h2 id=\"adding-image-input-support\" style=\"position:relative;\"><a href=\"#adding-image-input-support\" aria-label=\"adding image input support permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Adding Image Input Support</h2>\n<ul>\n<li>Adapters use cross-attention layers to process image representations within the language model.\n<ul>\n<li>Adapters are small neural network modules inserted into the layers of a pre-trained model</li>\n<li>Their primary purpose is to introduce new capabilities to the model, which is image processing in this case\n<ul>\n<li>Adapters are added between the layers of the pre-trained model</li>\n<li>They learn task-specific knowledge and adapt the outputs of each layer to the new task, which is image processing</li>\n</ul>\n</li>\n<li>In the context of Llama3.2, the adapters connect the image encoder to the language model using cross-attention mechanisms.</li>\n</ul>\n</li>\n<li>Introduces adapter weights to integrate a pre-trained image encoder into the language model.\n<ul>\n<li>Adapter weights refer to the learnable parameters within the adapter modules.</li>\n<li>These weights are adjusted to align the new input modality with the existing model while maintaining the core model</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"training-pipeline\" style=\"position:relative;\"><a href=\"#training-pipeline\" aria-label=\"training pipeline permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Training Pipeline</h2>\n<h3 id=\"pre-training\" style=\"position:relative;\"><a href=\"#pre-training\" aria-label=\"pre training permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Pre-Training:</h3>\n<ul>\n<li>Starts with pre-trained Llama3.1 text models.</li>\n<li>Image adapters and encoders are added.</li>\n<li>Pre-training uses a large-scale dataset of noisy image-text pairs.\n<ul>\n<li><code class=\"language-text\">Noisy</code> image-text pairs refer to datasets where the association between images and their corresponding textual descriptions is imperfect</li>\n<li>It is not that the images are labeled with wrong descriptions, its more like descriptions with less detail, and the model is supposed to find the detailed descriptions through training</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"during-training\" style=\"position:relative;\"><a href=\"#during-training\" aria-label=\"during training permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>During training:</h3>\n<ul>\n<li>Image encoder parameters are updated.</li>\n<li>Language model parameters remain unchanged, preserving Llama3.1’s original text-only capabilities.</li>\n<li>Instead of using the <code class=\"language-text\">noisy</code> image-text pairs used in pre-training, medium-scale high quality in-domain and knowledge-enhanced image-text pair data is used\n<ul>\n<li>This approach helps refine the model’s understanding and performance, similar to how knowledge distillation leverages refined outputs to improve model training.</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"fine-tuning\" style=\"position:relative;\"><a href=\"#fine-tuning\" aria-label=\"fine tuning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Fine-Tuning:</h3>\n<ul>\n<li>Uses a medium-scale dataset of high-quality, in-domain, and knowledge-enhanced image-text pairs.</li>\n<li>to refine the model’s capabilities for specific tasks and improve its overall performance</li>\n</ul>\n<h3 id=\"post-training-and-alignment\" style=\"position:relative;\"><a href=\"#post-training-and-alignment\" aria-label=\"post training and alignment permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Post-Training and Alignment</h3>\n<ul>\n<li><strong>Supervised Fine-Tuning</strong>: Conducts rounds of fine-tuning using supervised learning.</li>\n<li><strong><a href=\"https://arxiv.org/abs/2309.06657\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Rejection Sampling</a></strong>: Applies sampling methods to select the best-performing outputs.\n<ul>\n<li>a technique used to filter or modify model outputs by setting specific criteria that the outputs must meet</li>\n<li>used when the outputs may not align with desired constraints or quality standards\n<ul>\n<li>I believe this was crucial as the model size is really small. Meta must have set some sort of standards for the smaller models to meet before releasing them</li>\n</ul>\n</li>\n<li>this also helps with safety measures</li>\n</ul>\n</li>\n<li><strong><a href=\"https://arxiv.org/abs/2305.18290\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Direct Preference Optimization(DPO)</a></strong>: Directly optimizes model outputs for better performance.\n<ul>\n<li>this goes well with the rejection sampling, as the purpose of DPO is to align the outputs of the model with the standards that had been previously set</li>\n<li>uses preference data to adjust the model, reducing potential noise introduced by the reward model.</li>\n<li>this will be the topic of the next post</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"synthetic-data-generation\" style=\"position:relative;\"><a href=\"#synthetic-data-generation\" aria-label=\"synthetic data generation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Synthetic Data Generation</h2>\n<ul>\n<li>Employs Llama3.1 to filter and augment questions and answers based on in-domain images.\n<ul>\n<li>According to the <a href=\"https://ai.meta.com/blog/meta-llama-3-1/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Llama3.1 Release Note</a>, Llama3.1 405B is supposed to be good at synthetic data generation</li>\n</ul>\n</li>\n<li>Uses a reward model to rank candidate answers, creating high-quality fine-tuning data.\n<ul>\n<li>this aligns with the principle of Direct Preference Optimization(DPO)</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"safety-and-agentic-capabilities\" style=\"position:relative;\"><a href=\"#safety-and-agentic-capabilities\" aria-label=\"safety and agentic capabilities permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Safety and Agentic Capabilities</h2>\n<ul>\n<li>Includes safety mitigation data to maintain the model’s helpfulness and ensure safe operation.</li>\n<li>Enables deeper understanding and reasoning with multimodal (image and text) inputs.</li>\n<li>Advances Llama models toward more sophisticated agentic capabilities.</li>\n</ul>\n<p><br><br></p>\n<h1 id=\"lightweight-models\" style=\"position:relative;\"><a href=\"#lightweight-models\" aria-label=\"lightweight models permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Lightweight models</h1>\n<ul>\n<li>Llama 3.2 introduces the first highly capable lightweight models (1B and 3B parameters) that can efficiently run on-device.</li>\n<li>These models are created using two key methods: pruning and distillation.</li>\n</ul>\n<h2 id=\"pruning\" style=\"position:relative;\"><a href=\"#pruning\" aria-label=\"pruning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Pruning:</h2>\n<ul>\n<li>Used structured pruning on the Llama 3.1 8B model to create smaller, efficient versions (1B and 3B).\n<ul>\n<li>Pruning is a technique used to reduce the size of a neural network by removing less important or redundant parameters (weights, neurons, or filters) while attempting to maintain the model’s performance.</li>\n<li>The goal of pruning is to make the model more efficient, reducing its memory footprint, computational requirements, and potentially speeding up inference times</li>\n<li>When a model is trained, not all the neurons or parameters contribute equally to its final performance. Pruning identifies and removes those less critical components, leading to a smaller and more optimized model.</li>\n</ul>\n</li>\n<li>Involves systematically removing parts of the network while adjusting the remaining weights and gradients.</li>\n<li>Aims to reduce model size while retaining as much of the original model’s knowledge and performance as possible.</li>\n</ul>\n<h2 id=\"knowledge-distillation\" style=\"position:relative;\"><a href=\"#knowledge-distillation\" aria-label=\"knowledge distillation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Knowledge Distillation:</h2>\n<ul>\n<li>Uses a larger model (Llama 3.1 8B and 70B) as a “teacher” to transfer knowledge to the smaller 1B and 3B models.</li>\n<li>Incorporated logits from the larger models during the pre-training stage, using these outputs as token-level targets.</li>\n<li>Applied after pruning to help the smaller models recover performance.</li>\n<li>I have written about pruning and knowledge distillation in this <a href=\"https://jasonkang14.github.io/ai/pruning-and-knowledge-distillation\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">post</a></li>\n</ul>\n<h1 id=\"post-training\" style=\"position:relative;\"><a href=\"#post-training\" aria-label=\"post training permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Post-Training</h1>\n<ul>\n<li>Scales context length support to 128K tokens while maintaining quality.</li>\n<li>Uses synthetic data generation, blending high-quality data for various capabilities like summarization, rewriting, instruction following, language reasoning, and tool use.</li>\n</ul>\n<p>The detailed examination of the release notes has heightened my interest in experimenting with Llama3.2. Meta’s emphasis on its enhanced capability to interpret charts and graphs suggests potential improvements in building a more robust Retrieval Augmented Generation (RAG) pipeline, particularly for processing PDF documents. In my upcoming analysis, I plan to explore Direct Preference Optimization (DPO) further and concurrently evaluate Llama3.2’s efficacy in data processing.</p>","fields":{"slug":"/posts/2024//llm/dissecting-llama-32","tagSlugs":["/tag/llm/"]},"frontmatter":{"date":"2024-10-27T20:35:37.121Z","description":"A shot at understanding Llama3.2","tags":["LLM"],"title":"Dissecting Llama3.2","socialImage":null}}},"pageContext":{"slug":"/posts/2024//llm/dissecting-llama-32"}},"staticQueryHashes":["251939775","288581551","401334301"]}