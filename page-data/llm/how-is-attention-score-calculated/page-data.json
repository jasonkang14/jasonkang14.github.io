{"componentChunkName":"component---src-templates-post-template-post-template-tsx","path":"/llm/how-is-attention-score-calculated","result":{"data":{"markdownRemark":{"id":"6b6f28e1-1df0-5a03-b338-35519d81f79a","html":"<p>In Korea, there is a tradition of “study” groups where engineers from various companies or backgrounds gather to explore specific engineering concepts. Participants typically select a book or an online curriculum to follow, and each week, one member is responsible for studying a chapter in depth and presenting the ideas to the group. I recently joined a study group focused on understanding how Large Language Models (LLMs) function. Last week, I was tasked with presenting the concepts of Attention and Transformers within LLMs. While I am familiar with these concepts, explaining them to others proved to be a distinct challenge compared to understanding them on my own. Here is my perspective on explaining how the Attension Score is calculated</p>\n<h1 id=\"attention-score란\" style=\"position:relative;\"><a href=\"#attention-score%EB%9E%80\" aria-label=\"attention score란 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Attention Score란?</h1>\n<p>Attention Score는 모델이 입력 데이터의 특정 부분에 더 집중하도록 가중치를 부여하는 방식이다. Attention Score는 주어진 Query와 Key의 관계를 수치화하여 계산되며, 이 값이 클수록 해당 Key가 Query와 더 밀접한 연관성을 가진다고 본다.</p>\n<h2 id=\"queryq-keyk-valuev-도출-과정\" style=\"position:relative;\"><a href=\"#queryq-keyk-valuev-%EB%8F%84%EC%B6%9C-%EA%B3%BC%EC%A0%95\" aria-label=\"queryq keyk valuev 도출 과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Query(Q), Key(K), Value(V) 도출 과정</h2>\n<p>Attention 메커니즘에서 Q(Query), K(Key), V(Value)는 입력 데이터에서 선형 변환(Linear Transformation)을 통해 얻어지는 벡터이다. Transformer 모델에서 입력은 보통 단어 임베딩(embedding)이나 그 변형된 형태로 표현되며, 이 벡터들이 $Q$, $K$, $V$로 매핑된다.</p>\n<p>예를들면 “어제 밤에 배가 고파서 치킨을 시켜서 먹었다”라는 문장에서, “배”라는 단어가</p>\n<ol>\n<li>과일 배</li>\n<li>사람의 배</li>\n<li>이동수단 배</li>\n</ol>\n<p>중에 어떤 것을 뜻하는지 결정하려면 <strong>Attention Score</strong>를 활용하여 문맥 정보를 기반으로 “배”와 관련된 단어들 간의 관계를 학습할 수 있다.</p>\n<br>\n<h2 id=\"1-입력-데이터-준비\" style=\"position:relative;\"><a href=\"#1-%EC%9E%85%EB%A0%A5-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A4%80%EB%B9%84\" aria-label=\"1 입력 데이터 준비 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong>1. 입력 데이터 준비</strong></h2>\n<p>문장을 단어 단위로 분리하고 각 단어를 임베딩 벡터로 변환한다.<br>\n예를 들어, 각 단어가 4차원 벡터로 표현된다고 하자:</p>\n<ul>\n<li>“어제” → [0.1, 0.2, 0.1, 0.3]</li>\n<li>“밤에” → [0.0, 0.1, 0.2, 0.4]</li>\n<li>“배” → [0.5, 0.3, 0.2, 0.1]</li>\n<li>“가” → [0.1, 0.1, 0.1, 0.2]</li>\n<li>“고파서” → [0.2, 0.3, 0.1, 0.0]</li>\n<li>“치킨을” → [0.4, 0.0, 0.3, 0.2]</li>\n<li>“시켜서” → [0.3, 0.1, 0.4, 0.1]</li>\n<li>“먹었다” → [0.5, 0.2, 0.0, 0.1]</li>\n</ul>\n<p>입력 행렬 $\\mathbf{X}$ 는 다음과 같다:\n$$\n\\mathbf{X} =\n\\begin{bmatrix}\n0.1 &#x26; 0.2 &#x26; 0.1 &#x26; 0.3 \\  % 어제\n0.0 &#x26; 0.1 &#x26; 0.2 &#x26; 0.4 \\  % 밤에\n0.5 &#x26; 0.3 &#x26; 0.2 &#x26; 0.1 \\  % 배\n0.1 &#x26; 0.1 &#x26; 0.1 &#x26; 0.2 \\  % 가\n0.2 &#x26; 0.3 &#x26; 0.1 &#x26; 0.0 \\  % 고파서\n0.4 &#x26; 0.0 &#x26; 0.3 &#x26; 0.2 \\  % 치킨을\n0.3 &#x26; 0.1 &#x26; 0.4 &#x26; 0.1 \\  % 시켜서\n0.5 &#x26; 0.2 &#x26; 0.0 &#x26; 0.1     % 먹었다\n\\end{bmatrix}\n$$</p>\n<br>\n<h2 id=\"2-q-k-계산\" style=\"position:relative;\"><a href=\"#2-q-k-%EA%B3%84%EC%82%B0\" aria-label=\"2 q k 계산 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong>2. Q, K 계산</strong></h2>\n<p>$\\mathbf{X}$를 Query(Q), Key(K) 로 변환하기 위해 학습 가능한 가중치 행렬(Weight Matrix) $W_Q, W_K$를 사용한다.</p>\n<p>편의상 Weight Matrix는 아래와 같이 정의한다.</p>\n<p>$$\n\\mathbf{W}_Q =\n\\begin{bmatrix}\n0.5 &#x26; 0.1 &#x26; 0.2 &#x26; 0.2 \\\n0.2 &#x26; 0.3 &#x26; 0.1 &#x26; 0.4 \\\n0.1 &#x26; 0.5 &#x26; 0.3 &#x26; 0.1 \\\n0.3 &#x26; 0.1 &#x26; 0.4 &#x26; 0.2\n\\end{bmatrix}, \\quad\n\\mathbf{W}_K =\n\\begin{bmatrix}\n0.4 &#x26; 0.2 &#x26; 0.1 &#x26; 0.3 \\\n0.1 &#x26; 0.3 &#x26; 0.2 &#x26; 0.5 \\\n0.2 &#x26; 0.4 &#x26; 0.5 &#x26; 0.1 \\\n0.3 &#x26; 0.2 &#x26; 0.1 &#x26; 0.4\n\\end{bmatrix}\n$$</p>\n<p>각 행렬 곱 연산을 통해 Q, K를 얻는다.</p>\n<p>$$\n\\mathbf{Q} = \\mathbf{X} \\mathbf{W}_Q, \\quad \\mathbf{K} = \\mathbf{X} \\mathbf{W}_K\n$$</p>\n<br>\n<h1 id=\"mathbfq-beginbmatrix01--02--01--03-00--01--02--04-05--03--02--01-01--01--01--02-02--03--01--00-04--00--03--02-03--01--04--01-05--02--00--01endbmatrixbeginbmatrix05--01--02--02-02--03--01--04-01--05--03--01-03--01--04--02endbmatrix\" style=\"position:relative;\"><a href=\"#mathbfq-beginbmatrix01--02--01--03-00--01--02--04-05--03--02--01-01--01--01--02-02--03--01--00-04--00--03--02-03--01--04--01-05--02--00--01endbmatrixbeginbmatrix05--01--02--02-02--03--01--04-01--05--03--01-03--01--04--02endbmatrix\" aria-label=\"mathbfq beginbmatrix01  02  01  03 00  01  02  04 05  03  02  01 01  01  01  02 02  03  01  00 04  00  03  02 03  01  04  01 05  02  00  01endbmatrixbeginbmatrix05  01  02  02 02  03  01  04 01  05  03  01 03  01  04  02endbmatrix permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>$$\n\\mathbf{Q} =\n\\begin{bmatrix}\n0.1 &#x26; 0.2 &#x26; 0.1 &#x26; 0.3 \\\n0.0 &#x26; 0.1 &#x26; 0.2 &#x26; 0.4 \\\n0.5 &#x26; 0.3 &#x26; 0.2 &#x26; 0.1 \\\n0.1 &#x26; 0.1 &#x26; 0.1 &#x26; 0.2 \\\n0.2 &#x26; 0.3 &#x26; 0.1 &#x26; 0.0 \\\n0.4 &#x26; 0.0 &#x26; 0.3 &#x26; 0.2 \\\n0.3 &#x26; 0.1 &#x26; 0.4 &#x26; 0.1 \\\n0.5 &#x26; 0.2 &#x26; 0.0 &#x26; 0.1\n\\end{bmatrix}\n\\begin{bmatrix}\n0.5 &#x26; 0.1 &#x26; 0.2 &#x26; 0.2 \\\n0.2 &#x26; 0.3 &#x26; 0.1 &#x26; 0.4 \\\n0.1 &#x26; 0.5 &#x26; 0.3 &#x26; 0.1 \\\n0.3 &#x26; 0.1 &#x26; 0.4 &#x26; 0.2\n\\end{bmatrix}</h1>\n<p>\\begin{bmatrix}\n0.17 &#x26; 0.24 &#x26; 0.17 &#x26; 0.22 \\\n0.17 &#x26; 0.28 &#x26; 0.23 &#x26; 0.20 \\\n0.36 &#x26; 0.27 &#x26; 0.26 &#x26; 0.25 \\\n0.11 &#x26; 0.15 &#x26; 0.13 &#x26; 0.14 \\\n0.16 &#x26; 0.18 &#x26; 0.12 &#x26; 0.16 \\\n0.28 &#x26; 0.16 &#x26; 0.26 &#x26; 0.24 \\\n0.31 &#x26; 0.18 &#x26; 0.27 &#x26; 0.25 \\\n0.32 &#x26; 0.19 &#x26; 0.20 &#x26; 0.22\n\\end{bmatrix}\n$$</p>\n<br>\n<h1 id=\"mathbfk-beginbmatrix01--02--01--03-00--01--02--04-05--03--02--01-01--01--01--02-02--03--01--00-04--00--03--02-03--01--04--01-05--02--00--01endbmatrixbeginbmatrix04--02--01--03-01--03--02--05-02--04--05--01-03--02--01--04endbmatrix\" style=\"position:relative;\"><a href=\"#mathbfk-beginbmatrix01--02--01--03-00--01--02--04-05--03--02--01-01--01--01--02-02--03--01--00-04--00--03--02-03--01--04--01-05--02--00--01endbmatrixbeginbmatrix04--02--01--03-01--03--02--05-02--04--05--01-03--02--01--04endbmatrix\" aria-label=\"mathbfk beginbmatrix01  02  01  03 00  01  02  04 05  03  02  01 01  01  01  02 02  03  01  00 04  00  03  02 03  01  04  01 05  02  00  01endbmatrixbeginbmatrix04  02  01  03 01  03  02  05 02  04  05  01 03  02  01  04endbmatrix permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>$$\n\\mathbf{K} =\n\\begin{bmatrix}\n0.1 &#x26; 0.2 &#x26; 0.1 &#x26; 0.3 \\\n0.0 &#x26; 0.1 &#x26; 0.2 &#x26; 0.4 \\\n0.5 &#x26; 0.3 &#x26; 0.2 &#x26; 0.1 \\\n0.1 &#x26; 0.1 &#x26; 0.1 &#x26; 0.2 \\\n0.2 &#x26; 0.3 &#x26; 0.1 &#x26; 0.0 \\\n0.4 &#x26; 0.0 &#x26; 0.3 &#x26; 0.2 \\\n0.3 &#x26; 0.1 &#x26; 0.4 &#x26; 0.1 \\\n0.5 &#x26; 0.2 &#x26; 0.0 &#x26; 0.1\n\\end{bmatrix}\n\\begin{bmatrix}\n0.4 &#x26; 0.2 &#x26; 0.1 &#x26; 0.3 \\\n0.1 &#x26; 0.3 &#x26; 0.2 &#x26; 0.5 \\\n0.2 &#x26; 0.4 &#x26; 0.5 &#x26; 0.1 \\\n0.3 &#x26; 0.2 &#x26; 0.1 &#x26; 0.4\n\\end{bmatrix}</h1>\n<p>\\begin{bmatrix}\n0.15 &#x26; 0.22 &#x26; 0.20 &#x26; 0.28 \\\n0.18 &#x26; 0.30 &#x26; 0.28 &#x26; 0.32 \\\n0.33 &#x26; 0.31 &#x26; 0.27 &#x26; 0.30 \\\n0.10 &#x26; 0.15 &#x26; 0.13 &#x26; 0.18 \\\n0.14 &#x26; 0.20 &#x26; 0.16 &#x26; 0.19 \\\n0.28 &#x26; 0.18 &#x26; 0.23 &#x26; 0.26 \\\n0.27 &#x26; 0.24 &#x26; 0.25 &#x26; 0.27 \\\n0.26 &#x26; 0.24 &#x26; 0.20 &#x26; 0.24\n\\end{bmatrix}\n$$</p>\n<br>\n<h2 id=\"3-attention-score-계산\" style=\"position:relative;\"><a href=\"#3-attention-score-%EA%B3%84%EC%82%B0\" aria-label=\"3 attention score 계산 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong>3. Attention Score 계산</strong></h2>\n<p>“배”의 의미를 결정하기 위해, “배”의 Query $(Q_{\\text{배}})$와 모든 단어의 Key $(K_{\\text{모든 단어}})$를 비교하여 <code class=\"language-text\">Attention Score</code>를 계산한다. Attention에서 <strong>Query(Q)</strong> 는 단어가 “묻고” 있거나 찾고 있는 것을 나타내고, <strong>Key(K)</strong> 는 각 단어가 제공하는 특징이나 문맥을 나타낸다. 따라서 아래와 같이 표현할 수 있다</p>\n<ul>\n<li>Query $(Q_\\text{배})$: “문맥적으로 나와 관련된 단어를 찾아보자.”</li>\n<li>Key $(K_\\text{밤에})$: “나는 시간적 정보를 제공하는 단어야.”</li>\n<li>Key $(K_\\text{치킨})$: “나는 음식과 관련된 단어야.”</li>\n</ul>\n<p><code class=\"language-text\">Attention Score</code>는 아래 공식을 통해 계산된다.</p>\n<p>$$\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n$$</p>\n<p>$d_k = 4$ 이기 때문에 $\\sqrt{d_k} = 2$가 된다.</p>\n<p>먼저 $\\mathbf{Q} \\mathbf{K}^T$를 계산하면:</p>\n<p>$$\n\\mathbf{Q} \\mathbf{K}^T =\n\\begin{bmatrix}\n0.17 &#x26; 0.24 &#x26; 0.17 &#x26; 0.22 \\\n0.17 &#x26; 0.28 &#x26; 0.23 &#x26; 0.20 \\\n0.36 &#x26; 0.27 &#x26; 0.26 &#x26; 0.25 \\\n0.11 &#x26; 0.15 &#x26; 0.13 &#x26; 0.14 \\\n0.16 &#x26; 0.18 &#x26; 0.12 &#x26; 0.16 \\\n0.28 &#x26; 0.16 &#x26; 0.26 &#x26; 0.24 \\\n0.31 &#x26; 0.18 &#x26; 0.27 &#x26; 0.25 \\\n0.32 &#x26; 0.19 &#x26; 0.20 &#x26; 0.22\n\\end{bmatrix}\n\\begin{bmatrix}\n0.15 &#x26; 0.18 &#x26; 0.33 &#x26; 0.10 &#x26; 0.14 &#x26; 0.28 &#x26; 0.27 &#x26; 0.26 \\\n0.22 &#x26; 0.30 &#x26; 0.31 &#x26; 0.15 &#x26; 0.20 &#x26; 0.18 &#x26; 0.24 &#x26; 0.24 \\\n0.20 &#x26; 0.28 &#x26; 0.27 &#x26; 0.13 &#x26; 0.16 &#x26; 0.23 &#x26; 0.25 &#x26; 0.20 \\\n0.28 &#x26; 0.32 &#x26; 0.30 &#x26; 0.18 &#x26; 0.19 &#x26; 0.26 &#x26; 0.27 &#x26; 0.24\n\\end{bmatrix}\n$$</p>\n<p>$$\n\\mathbf{Q} \\mathbf{K}^T =\n\\begin{bmatrix}\n0.156 &#x26; 0.212 &#x26; 0.199 &#x26; 0.119 &#x26; 0.158 &#x26; 0.211 &#x26; 0.229 &#x26; 0.217 \\\n0.170 &#x26; 0.236 &#x26; 0.220 &#x26; 0.131 &#x26; 0.175 &#x26; 0.233 &#x26; 0.248 &#x26; 0.230 \\\n0.274 &#x26; 0.374 &#x26; 0.342 &#x26; 0.204 &#x26; 0.266 &#x26; 0.352 &#x26; 0.375 &#x26; 0.340 \\\n0.114 &#x26; 0.154 &#x26; 0.143 &#x26; 0.085 &#x26; 0.110 &#x26; 0.147 &#x26; 0.158 &#x26; 0.144 \\\n0.131 &#x26; 0.174 &#x26; 0.159 &#x26; 0.095 &#x26; 0.126 &#x26; 0.166 &#x26; 0.176 &#x26; 0.159 \\\n0.239 &#x26; 0.315 &#x26; 0.288 &#x26; 0.173 &#x26; 0.227 &#x26; 0.302 &#x26; 0.321 &#x26; 0.293 \\\n0.264 &#x26; 0.346 &#x26; 0.316 &#x26; 0.189 &#x26; 0.249 &#x26; 0.328 &#x26; 0.349 &#x26; 0.319 \\\n0.258 &#x26; 0.339 &#x26; 0.311 &#x26; 0.186 &#x26; 0.243 &#x26; 0.320 &#x26; 0.340 &#x26; 0.313\n\\end{bmatrix}\n$$</p>\n<p>스케일링을 위해 $\\sqrt{d_k} = 2$로 나누면</p>\n<p>$$\n\\mathbf{QK^T}_{\\text{scaled}} =\n\\begin{bmatrix}\n0.078 &#x26; 0.106 &#x26; 0.100 &#x26; 0.060 &#x26; 0.079 &#x26; 0.106 &#x26; 0.115 &#x26; 0.109 \\\n0.085 &#x26; 0.118 &#x26; 0.110 &#x26; 0.065 &#x26; 0.087 &#x26; 0.117 &#x26; 0.124 &#x26; 0.115 \\\n0.137 &#x26; 0.187 &#x26; 0.171 &#x26; 0.102 &#x26; 0.133 &#x26; 0.176 &#x26; 0.188 &#x26; 0.170 \\\n0.057 &#x26; 0.077 &#x26; 0.071 &#x26; 0.043 &#x26; 0.055 &#x26; 0.074 &#x26; 0.079 &#x26; 0.072 \\\n0.065 &#x26; 0.087 &#x26; 0.080 &#x26; 0.048 &#x26; 0.063 &#x26; 0.083 &#x26; 0.088 &#x26; 0.080 \\\n0.120 &#x26; 0.158 &#x26; 0.144 &#x26; 0.087 &#x26; 0.114 &#x26; 0.151 &#x26; 0.160 &#x26; 0.146 \\\n0.132 &#x26; 0.173 &#x26; 0.158 &#x26; 0.095 &#x26; 0.125 &#x26; 0.164 &#x26; 0.175 &#x26; 0.160 \\\n0.129 &#x26; 0.170 &#x26; 0.156 &#x26; 0.093 &#x26; 0.121 &#x26; 0.160 &#x26; 0.170 &#x26; 0.157\n\\end{bmatrix}\n$$</p>\n<p>위에 scaling된 Matrix를 Attention Score라고 부른다.\n이제 <code class=\"language-text\">softmax</code>를 적용한다. 우리는 “배”라는 단어가 무엇을 뜻하는지에 관심이 있기 때문에 3번 row를 대상으로 대표 계산을 진행하고 <code class=\"language-text\">softmax</code>를 적용한다.</p>\n<p>$$\n\\text{Softmax}(\\text{Row 3}) = \\frac{\\exp(x_i)}{\\sum_{j}\\exp(x_j)}, \\quad x_i \\in \\text{Row 3}\n$$</p>\n<p>$$\n\\exp(\\text{Row 3}) = \\begin{bmatrix}\n1.147 &#x26; 1.205 &#x26; 1.187 &#x26; 1.108 &#x26; 1.143 &#x26; 1.193 &#x26; 1.207 &#x26; 1.186\n\\end{bmatrix}\n$$</p>\n<p>각 항목의 합을 구하면</p>\n<p>$$\n\\text{sum} = 1.147 + 1.205 + 1.187 + 1.108 + 1.143 + 1.193 + 1.207 + 1.186 = 9.376\n$$</p>\n<p>sum을 활용해 normalization을 적용하면,</p>\n<p>$$\n\\text{Softmax}(\\text{Row 3}) = \\begin{bmatrix}\n0.122 &#x26; 0.129 &#x26; 0.127 &#x26; 0.118 &#x26; 0.122 &#x26; 0.127 &#x26; 0.129 &#x26; 0.127\n\\end{bmatrix}\n$$</p>\n<p>다른 row들에도 모두 softmax를 적용하면 아래 Attention Weights가 계산된다.</p>\n<p>$$\n\\mathbf{AW_i} =\n\\begin{bmatrix}\n0.123 &#x26; 0.127 &#x26; 0.126 &#x26; 0.121 &#x26; 0.123 &#x26; 0.127 &#x26; 0.128 &#x26; 0.127 \\\n0.123 &#x26; 0.127 &#x26; 0.126 &#x26; 0.120 &#x26; 0.123 &#x26; 0.127 &#x26; 0.128 &#x26; 0.126 \\\n0.122 &#x26; 0.129 &#x26; 0.127 &#x26; 0.118 &#x26; 0.122 &#x26; 0.127 &#x26; 0.129 &#x26; 0.127 \\\n0.124 &#x26; 0.127 &#x26; 0.126 &#x26; 0.122 &#x26; 0.124 &#x26; 0.126 &#x26; 0.127 &#x26; 0.126 \\\n0.124 &#x26; 0.127 &#x26; 0.126 &#x26; 0.122 &#x26; 0.124 &#x26; 0.126 &#x26; 0.127 &#x26; 0.126 \\\n0.122 &#x26; 0.127 &#x26; 0.125 &#x26; 0.118 &#x26; 0.122 &#x26; 0.126 &#x26; 0.127 &#x26; 0.125 \\\n0.123 &#x26; 0.128 &#x26; 0.126 &#x26; 0.118 &#x26; 0.122 &#x26; 0.127 &#x26; 0.128 &#x26; 0.126 \\\n0.123 &#x26; 0.128 &#x26; 0.126 &#x26; 0.119 &#x26; 0.122 &#x26;</p>\n<p>0.127 &#x26; 0.128 &#x26; 0.126\n\\end{bmatrix}\n$$</p>\n<br>\n<h2 id=\"4-최종-output-계산\" style=\"position:relative;\"><a href=\"#4-%EC%B5%9C%EC%A2%85-output-%EA%B3%84%EC%82%B0\" aria-label=\"4 최종 output 계산 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong>4. 최종 Output 계산</strong></h2>\n<p>Attention Weight를 Value (V)에 곱하여 최종 Output을 계산한다:\n$$\n\\text{Output}<em>{\\text{배}} = \\sum</em>{i} \\text{Attention Weight}_i \\cdot V_i\n$$</p>\n<p>Value(V)는 각 단어가 “전달할 정보”를 담고 있는 벡터이다.<br>\n앞서 입력 행렬 $\\mathbf{X}$ 와 학습된 $W_V$를 사용해 V를 계산하면 다음과 같다:</p>\n<p>$$\n\\mathbf{W}_V =\n\\begin{bmatrix}\n0.3 &#x26; 0.1 &#x26; 0.2 &#x26; 0.4 \\\n0.1 &#x26; 0.4 &#x26; 0.3 &#x26; 0.2 \\\n0.4 &#x26; 0.2 &#x26; 0.1 &#x26; 0.3 \\\n0.2 &#x26; 0.3 &#x26; 0.4 &#x26; 0.1\n\\end{bmatrix}\n$$</p>\n<h1 id=\"mathbfv--mathbfx-mathbfw_v-beginbmatrix01--02--01--03-00--01--02--04-05--03--02--01-01--01--01--02-02--03--01--00-04--00--03--02-03--01--04--01-05--02--00--01endbmatrixbeginbmatrix03--01--02--04-01--04--03--02-04--02--01--03-02--03--04--01endbmatrix\" style=\"position:relative;\"><a href=\"#mathbfv--mathbfx-mathbfw_v-beginbmatrix01--02--01--03-00--01--02--04-05--03--02--01-01--01--01--02-02--03--01--00-04--00--03--02-03--01--04--01-05--02--00--01endbmatrixbeginbmatrix03--01--02--04-01--04--03--02-04--02--01--03-02--03--04--01endbmatrix\" aria-label=\"mathbfv  mathbfx mathbfw_v beginbmatrix01  02  01  03 00  01  02  04 05  03  02  01 01  01  01  02 02  03  01  00 04  00  03  02 03  01  04  01 05  02  00  01endbmatrixbeginbmatrix03  01  02  04 01  04  03  02 04  02  01  03 02  03  04  01endbmatrix permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>$$\n\\mathbf{V} = \\mathbf{X} \\mathbf{W}_V\n= \\begin{bmatrix}\n0.1 &#x26; 0.2 &#x26; 0.1 &#x26; 0.3 \\\n0.0 &#x26; 0.1 &#x26; 0.2 &#x26; 0.4 \\\n0.5 &#x26; 0.3 &#x26; 0.2 &#x26; 0.1 \\\n0.1 &#x26; 0.1 &#x26; 0.1 &#x26; 0.2 \\\n0.2 &#x26; 0.3 &#x26; 0.1 &#x26; 0.0 \\\n0.4 &#x26; 0.0 &#x26; 0.3 &#x26; 0.2 \\\n0.3 &#x26; 0.1 &#x26; 0.4 &#x26; 0.1 \\\n0.5 &#x26; 0.2 &#x26; 0.0 &#x26; 0.1\n\\end{bmatrix}\n\\begin{bmatrix}\n0.3 &#x26; 0.1 &#x26; 0.2 &#x26; 0.4 \\\n0.1 &#x26; 0.4 &#x26; 0.3 &#x26; 0.2 \\\n0.4 &#x26; 0.2 &#x26; 0.1 &#x26; 0.3 \\\n0.2 &#x26; 0.3 &#x26; 0.4 &#x26; 0.1\n\\end{bmatrix}</h1>\n<p>\\begin{bmatrix}\n0.17 &#x26; 0.18 &#x26; 0.18 &#x26; 0.18 \\\n0.22 &#x26; 0.20 &#x26; 0.18 &#x26; 0.18 \\\n0.32 &#x26; 0.26 &#x26; 0.23 &#x26; 0.21 \\\n0.13 &#x26; 0.13 &#x26; 0.13 &#x26; 0.12 \\\n0.17 &#x26; 0.17 &#x26; 0.15 &#x26; 0.12 \\\n0.30 &#x26; 0.21 &#x26; 0.23 &#x26; 0.20 \\\n0.32 &#x26; 0.23 &#x26; 0.24 &#x26; 0.21 \\\n0.29 &#x26; 0.22 &#x26; 0.20 &#x26; 0.18\n\\end{bmatrix}\n$$</p>\n<p>이제 “배”에 대한 Attention Output을 계산한다</p>\n<p>$$\n\\mathbf{A}_{\\text{Row 3}} \\times \\mathbf{V} =\n[0.122, 0.129, 0.127, 0.118, 0.122, 0.127, 0.129, 0.127] \\times\n\\begin{bmatrix}\n0.17 &#x26; 0.18 &#x26; 0.18 &#x26; 0.18 \\\n0.22 &#x26; 0.20 &#x26; 0.18 &#x26; 0.18 \\\n0.32 &#x26; 0.26 &#x26; 0.23 &#x26; 0.21 \\\n0.13 &#x26; 0.13 &#x26; 0.13 &#x26; 0.12 \\\n0.17 &#x26; 0.17 &#x26; 0.15 &#x26; 0.12 \\\n0.30 &#x26; 0.21 &#x26; 0.23 &#x26; 0.20 \\\n0.32 &#x26; 0.23 &#x26; 0.24 &#x26; 0.21 \\\n0.29 &#x26; 0.22 &#x26; 0.20 &#x26; 0.18\n\\end{bmatrix} =\n$$</p>\n<p>$$\n\\mathbf{A}_{\\text{Row 3}} \\times \\mathbf{V} =\n[0.275, 0.222, 0.206, 0.183]\n$$</p>\n<p>전체 row에 대해 계산하면 아래와 같은 Attention Output 결과를 볼 수 있다\n$$\n\\text{Attention Output} =\n\\begin{bmatrix}\n0.267 &#x26; 0.219 &#x26; 0.204 &#x26; 0.181 \\\n0.267 &#x26; 0.219 &#x26; 0.204 &#x26; 0.181 \\\n0.275 &#x26; 0.222 &#x26; 0.206 &#x26; 0.183 \\\n0.263 &#x26; 0.218 &#x26; 0.203 &#x26; 0.180 \\\n0.263 &#x26; 0.218 &#x26; 0.203 &#x26; 0.180 \\\n0.270 &#x26; 0.221 &#x26; 0.205 &#x26; 0.182 \\\n0.272 &#x26; 0.221 &#x26; 0.205 &#x26; 0.182 \\\n0.271 &#x26; 0.220 &#x26; 0.204 &#x26; 0.181\n\\end{bmatrix}\n$$\n<br></p>\n<h2 id=\"5-attention-score를-통한-의미-결정\" style=\"position:relative;\"><a href=\"#5-attention-score%EB%A5%BC-%ED%86%B5%ED%95%9C-%EC%9D%98%EB%AF%B8-%EA%B2%B0%EC%A0%95\" aria-label=\"5 attention score를 통한 의미 결정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong>5. Attention Score를 통한 의미 결정</strong></h2>\n<p>최종 Output [0.73, 0.32]는 “배”가 문맥에서 가장 중요한 단어인 “고파서”와 강하게 연결된 정보를 반영한다. Output이 “배”가 “고파서”와 가장 강하게 연결됐다는 Attention Score를 활용해서 계산된 결과이기 때문이다</p>\n<p>이 Output은 다음과 같은 역할을 한다:</p>\n<ol>\n<li><strong>문맥적 의미 제공</strong>: “배”가 “고파서”와 연관되어 “사람의 배”로 해석될 가능성이 높다는 문맥적 정보를 포함한다.</li>\n<li><strong>후속 레이어 활용</strong>: Transformer의 다음 레이어는 이 정보를 개반으로 “배”의 문맥적 역할을 더 정교하게 학습한다.</li>\n<li><strong>최종 작업에 반영</strong>: 번역, 요약, 질문응답 등 NLP 작업에서 “배”의 문맥적 의미가 정확히 반영되도록 지원한다.</li>\n</ol>\n<p>최종 Output 벡터는 Transformer의 후속 레이어에서 사용되며, 다음과 같은 방식으로 “배”의 의미를 결정한다:</p>\n<ol>\n<li>\n<p><strong>문맥 기반 표현 학습</strong>: Output 벡터는 “배”가 다른 단어와의 문맥적 관계를 학습한 결과다.</p>\n<ul>\n<li>“배”와 “고파서”, “치킨”의 관계 → “사람의 배”일 가능성이 높음.</li>\n<li>“배”와 “밤에”, “치킨”의 관계 → “과일 배”일 가능성.</li>\n<li>“배”와 “밤에”의 관계 → 높진 않지만 “이동수단 배”일 가능성.</li>\n</ul>\n</li>\n<li>\n<p><strong>다음 작업 수행</strong>:</p>\n<ul>\n<li>질문 답변(Task): “배”가 무엇인지 명확히 설명.</li>\n<li>번역(Task): “배”를 상황에 따라 다른 언어로 번역.</li>\n</ul>\n</li>\n</ol>\n<p>이제 Attention Score가 어떻게 계산되는지 이해했으니, 이를 기반으로 Transformer 모델이 어떻게 학습되는지 알아보도록 하겠다.</p>","fields":{"slug":"/posts/2024//llm/how-is-attention-score-calculated","tagSlugs":["/tag/llm/"]},"frontmatter":{"date":"2024-12-01T20:35:37.121Z","description":"A detailed exploration of how attentions are calculated in the Transformer model, as introduced in 'Attention Is All You Need.'","tags":["LLM"],"title":"How Is Attention Calculated?","socialImage":null}}},"pageContext":{"slug":"/posts/2024//llm/how-is-attention-score-calculated"}},"staticQueryHashes":["251939775","288581551","401334301"]}