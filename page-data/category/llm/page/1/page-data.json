{"componentChunkName":"component---src-templates-category-template-category-template-tsx","path":"/category/llm/page/1","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"fields":{"slug":"/posts/2024//llm/how-to-use-llm-as-a-reranker","categorySlug":"/category/llm/"},"frontmatter":{"description":"Explore the concept of Rerankers, their role in enhancing search results, and how they leverage large language models to improve the relevance and accuracy of information retrieval.","category":"LLM","title":"Using LLM as a Reranker","date":"2024-12-19T20:35:37.121Z","slug":"/llm/how-to-use-llm-as-a-reranker"}}},{"node":{"fields":{"slug":"/posts/2024//llm/how-is-attention-score-calculated","categorySlug":"/category/llm/"},"frontmatter":{"description":"A detailed exploration of how attentions are calculated in the Transformer model, as introduced in 'Attention Is All You Need.'","category":"LLM","title":"How Is Attention Calculated?","date":"2024-12-01T20:35:37.121Z","slug":"/llm/how-is-attention-score-calculated"}}},{"node":{"fields":{"slug":"/posts/2024//llm/what-is-attention","categorySlug":"/category/llm/"},"frontmatter":{"description":"An exploration of the concept of Attention in LLMs, discussing its significance and impact on model performance and understanding.","category":"LLM","title":"What Is Attention?","date":"2024-11-30T20:35:37.121Z","slug":"/llm/what-is-attention"}}},{"node":{"fields":{"slug":"/posts/2024//llm/why-evaluation-is-important","categorySlug":"/category/llm/"},"frontmatter":{"description":"Exploring the significance of evaluation in developing LLM applications","category":"LLM","title":"Why Is Evaluation Important in Building an LLM Application?","date":"2024-11-21T20:35:37.121Z","slug":"/llm/why-evaluation-is-important"}}}]}},"pageContext":{"group":"LLM","limit":4,"offset":4,"pagination":{"currentPage":1,"prevPagePath":"/category/llm","nextPagePath":"/category/llm/page/2","hasNextPage":true,"hasPrevPage":true}}},"staticQueryHashes":["251939775","288581551","401334301","63107425"]}