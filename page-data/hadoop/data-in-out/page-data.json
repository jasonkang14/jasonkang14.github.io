{"componentChunkName":"component---src-templates-post-template-post-template-tsx","path":"/hadoop/data-in-out","result":{"data":{"markdownRemark":{"id":"dcafd980-af8d-5a1f-bd97-a35670e44660","html":"<h1 id=\"data-integrity\" style=\"position:relative;\"><a href=\"#data-integrity\" aria-label=\"data integrity permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Data Integrity</h1>\n<p>하둡처럼 방대한 양의 데이터를 처리할 경우에는, 데이터가 분실되거나 corrupt될 가능성이 높다. 데이터가 corrupt된 것을 탐지하기 위한 가장 일반적인 방법은 <code class=\"language-text\">checksum</code>을 계산하는 것인데, 데이터가 처음 하둡에 들어왔을 때 실시하고, 데이터가 이동하거나 corrupt될 가능성이 있는 경우에 다시 실시한다. 하지만 이 방법은 에러가 있는지 확인하는 절차에 불과하고, corrupt된 데이터를 회복시킬수는 없다. 데이터가 아니라 <code class=\"language-text\">checksum</code>이 corrupt되었을 수도 있지만 이 가능성은 매우 낮다.</p>\n<p><code class=\"language-text\">checksum</code>은 데이터의 authenticity를 판단하는 방법인데 아래와 같은 흐름으로 이루어진다.</p>\n<p><img src=\"https://i.imgur.com/X4NoYCD.png\" alt=\"how-checksum-works\"></p>\n<p>일반적으로 32-bit 정수로 이루어진 <code class=\"language-text\">checksum</code>을 compute한다.</p>\n<h3 id=\"data-integrity-in-hdfs\" style=\"position:relative;\"><a href=\"#data-integrity-in-hdfs\" aria-label=\"data integrity in hdfs permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Data Integrity in HDFS</h3>\n<p>HDFS는 데이터가 write될 때 checksum을 실시하고, 데이터를 read할 때 checksum을 verify한다. checksum의 default 크기는 512bytes이다. <code class=\"language-text\">datanode</code>가 클라이언트로부터 받은 데이터를 저장하거나, 어떤 데이터를 복제할 때마다 checksum을 실시한다. <code class=\"language-text\">datanode</code>가 checksum을 실시하다가 실패하면 클라이언트에게 <code class=\"language-text\">IOException</code>을 리턴하고, 클라이언트는 application상태에 따라 이 exception을 처리한다.</p>\n<p>클라이언트가 데이터를 read할 때 checksum을 verify하기도 한다. 각각의 node들은 checksum의 기록을 남겨서 특정 block이 가장 최근에 verify된 시점을 확인한다. 또한 각 <code class=\"language-text\">datanode</code>는 백그라운드 thread의 <code class=\"language-text\">DataBlockScanner</code>를 통해서 주기적으로 block들을 verify한다.</p>\n<p>HDFS가 데이터를 replication해서 저장 하기 때문에, 특정 block이 corrupt될 경우에 corrupt되지 않은 replica를 사용해서 해당 block을 복구할 수 있다. 클라이언트가 corrupt된 block을 발견하면, <code class=\"language-text\">ChecksumException</code>이 발생하기 전에 <code class=\"language-text\">namenode</code>로 부터 다른 block을 read할 수 있도록 요청한다. <code class=\"language-text\">namenode</code>는 다른 클라이언트들이 corrupt된 block에 접근할 수 없도록 처리하고, replication level을 유지하기 위해 다른 <code class=\"language-text\">datanode</code>에 block을 복제하고 corrupt된 block은 삭제한다. corrupt된 데이터를 확인하고 싶다면 checksum을 bypass하는 옵션을 추가할 숭 ㅣㅆ다.</p>\n<h3 id=\"localfilesystem\" style=\"position:relative;\"><a href=\"#localfilesystem\" aria-label=\"localfilesystem permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>LocalFileSystem</h3>\n<p>하둡의 <code class=\"language-text\">LocalFileSystem</code>은 클라이언트에서 checksum을 실시한다.</p>\n<p>클라이언트가 <code class=\"language-text\">filename</code>이라는 파일을 write할 때, <code class=\"language-text\">LocalFileSystem</code>은 해당 파일의 checksum과 같은 디렉토리에 <code class=\"language-text\">.filename.crc</code>라는 파일을 생성한다. checksum의 chunk size는 <code class=\"language-text\">.crc</code>파일의 metadata에 저장되어 chunk size가 변하더라도 read할 수 있도록 한다. checksum은 해당 파일을 read할 때 verify되고, 만약 문제가 있다면 <code class=\"language-text\">ChecksumException</code>을 return한다.</p>\n<p>checksum을 실시하는 비용은 낮은 퍼센트의 오버헤드만 추가될 뿐 높지 않은 편이다. data integrity를 보장하기 위해서라면 이정도는 쓸만하다. 뒷단에 있는 filesystem이 checksum을 지원한다면 <code class=\"language-text\">LocalFileSystem</code>대신 <code class=\"language-text\">RawFileSystem</code>을 사용해서 checksum을 disable할 수도 있다.</p>\n<h3 id=\"checksumfilesystem\" style=\"position:relative;\"><a href=\"#checksumfilesystem\" aria-label=\"checksumfilesystem permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ChecksumFileSystem</h3>\n<p><code class=\"language-text\">LocalFileSystem</code>은 <code class=\"language-text\">ChecksumFileSystem</code>을 사용해서 checksum을 실시한다. <code class=\"language-text\">ChecksumFileSystem</code> class를 사용하면 다른 filesystem들에 checksum을 쉽게 도입할 수 있다. 데이터가 corrupt된 경우 <code class=\"language-text\">ChecksumFileSystem</code>이 실패 flag를 만들면, <code class=\"language-text\">LocalFileSystem</code>이 문제가 있는 파일과 그 checksum들을 <code class=\"language-text\">bad_files</code> 라는 디렉토리로 옮긴다.</p>\n<h1 id=\"compression\" style=\"position:relative;\"><a href=\"#compression\" aria-label=\"compression permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Compression</h1>\n<p>파일 압축은 두가지 장점이 있는데</p>\n<ol>\n<li>파일을 저장하는 공간을 줄일 수 있고,</li>\n<li>디스크 간 네트워크를 사용해서 데이터 전송을 빠르게 할 수 있다.</li>\n</ol>\n<p>압축에 대한 다양한 방법들은 아래 표에 상세히 나와있다\n<img src=\"https://i.imgur.com/CY8ldEr.png\" alt=\"how-hadoop-compresses-files\"></p>\n<p>위에서 언급한 두 장점은 서로 상쇄하는 문제(?)가 있는데, 압축과 푸는 속도가 빠르다면 공간을 많이 아낄 수 없다. 그리고 공간을 많이 아낀다면 압축하는데 시간이 오래 걸린다.</p>\n<h3 id=\"codecs\" style=\"position:relative;\"><a href=\"#codecs\" aria-label=\"codecs permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Codecs</h3>\n<p><code class=\"language-text\">codec</code>은 압축했다가 풀었다가 하는 알고리즘이다. 하둡에서는 <code class=\"language-text\">CompressionCodec</code> interface로 구현되어 있다. 책에는 예제 코드와 함께 여러가지를 설명한다.</p>\n<h3 id=\"compression-and-input-splits\" style=\"position:relative;\"><a href=\"#compression-and-input-splits\" aria-label=\"compression and input splits permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Compression and Input Splits</h3>\n<p><code class=\"language-text\">MapReduce</code> job을 위해 데이터를 압축한다면, 압축 알고리즘이 각 block을 split할 수 있는지를 고려해야한다. 예를들면 1GB의 데이터가 있다면, HDFS가 128MB사이즈의 8개 block들로 나눠서 저장한다. <code class=\"language-text\">MapReduce</code>는 이 block들을 다시 8개의 input stream으로 나누어서 mapper에 넣으면 된다. 하지만 해당 데이터를 gzip으로 압축시키면 HDFS는 8개의 block으로 데이터를 저장하겠지만, gzip의 stream의 내의 임의의 포인트에서 데이터를 read할 수 없기 때문에 각각의 block을 split 할 수 없다.</p>\n<p>gzip은 <code class=\"language-text\">DEFLATE</code>라는 방식으로 압축된 데이터를 저장하는데, <code class=\"language-text\">DEFALTE</code> 방식은 각 block의 시작점을 구분할 수 없다는 문제점이 있다. 그래서 gzip은 block split을 지원하지 않는다. 이런 경우에 <code class=\"language-text\">MapReduce</code>는 각 block을 split하지 않는다. 하지만 이 경우 locality를 위반한다. 왜냐면 하나의 mapper가 8개의 block을 처리해야 하기 때문인데, 이 block들이 mapper와 같은 위치에 존재할 가능성이 매우 떨어진다. 반면 <code class=\"language-text\">LZO</code>를 사용해서 압축하면 <code class=\"language-text\">MapReduce</code> job을 위해 데이터를 split할 수 있다.</p>\n<h1 id=\"serialization\" style=\"position:relative;\"><a href=\"#serialization\" aria-label=\"serialization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Serialization</h1>\n<p><code class=\"language-text\">Serialization</code>은 object를 byte stream으로 변환하여 네트워크로 전송하거나 persistant storage에 저장할 수 있도록 변환하는 방법이다. <code class=\"language-text\">Deserialization</code>은 반대로 bytestream을 object로 변환한다.</p>\n<p>데이터 분산과 처리에 <code class=\"language-text\">serialization</code>이 사용되는 경우는 process간의 소통과 persistant storage이다.</p>\n<p>하둡에서는 process간의 소통은 Remote Procedure Calls(RPC)로 구현되었다. RPC 프로토콜은 메세지를 binary stream을 다른 node로 보내기 위해 <code class=\"language-text\">serialization</code>을 사용한다. binary stream을 받은 node는 <code class=\"language-text\">deserialization</code>을 통해 메세지를 확인한다. 따라서 serizliation 포맷은 간결하고, 빠르고, 확장성이 있으며, 상호 작동할 수 있어야한다.</p>\n<p>하둡은 자체 <code class=\"language-text\">serialization</code> 포맷인 <code class=\"language-text\">Writables</code>를 사용한다. 매우 간결하고 빠르지만, Java이외의 언어로는 사용할 수 없다는 단점이 있다.</p>\n<p><code class=\"language-text\">Writable</code> interface에는 두가지 method들이 있다. 하나는 상태를 <code class=\"language-text\">DataOutput</code>의 binary stream에 write하는 것이고, 다른 하나는 <code class=\"language-text\">DataInput</code> binary stream으로 부터 상태를 read하는 것이다. <code class=\"language-text\">serialization</code>과 <code class=\"language-text\">deserialization</code>이 가능하다고 보면 되겠다. <code class=\"language-text\">Writable</code>도 다양한 종류의 class들이 있는데 원시 타입들마다 다른 class를 사용한다.</p>\n<h1 id=\"file-based-data-structures\" style=\"position:relative;\"><a href=\"#file-based-data-structures\" aria-label=\"file based data structures permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>File-Based Data Structures</h1>\n<p>가끔 데이터를 저장하기 위해 특별한 자료구조를 필요로 할 때가 있다. <code class=\"language-text\">MapReduce</code> job을 처리할 때, binary data의 blob별로 파일을 만들어서 저장하는 것은 scalability가 떨어지기 때문에 하둡은 다양한 higher-level container들을 갖고 있다.</p>\n<h3 id=\"sequencefile\" style=\"position:relative;\"><a href=\"#sequencefile\" aria-label=\"sequencefile permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>SequenceFile</h3>\n<p>한 줄짜리 텍스트로 이루어진 로그파일이 있다고 가정한다. binary type을 로깅한다면 plain text는 적합하지 않다. 따라서 하둡의 <code class=\"language-text\">SequenceFile</code> class는 binary key-value pair를 저장하기 위한 자료구조를 제공한다. <code class=\"language-text\">SequenceFile</code> class는 작은 파일들을 저장하는데도 유리하다. <code class=\"language-text\">HDFS</code>와 <code class=\"language-text\">MapReduce</code>는 큰 파일들을 처리하는 것에 최적화 되어 있기 때문에 큰 파일들을 <code class=\"language-text\">SequenceFile</code>로 packing하면 작은 파일들을 더 효율적으로 저장할 수 있다.</p>\n<p><code class=\"language-text\">SequenceFile.Writer</code> 인스턴스를 사용해서 <code class=\"language-text\">SequenceFile</code>을 write 할 수 있다. write를 실행할 stream과 configuration, key-value의 type과 같은 필수 항목들을 지정해주면 된다. 압축 타입과 codec, write중에 실행할 callback, metadata정보들도 optional하게 넣어줄 수 있다. write를 하려면 해당 인스턴스에서 계속 <code class=\"language-text\">.append()</code>해주면 된다.</p>\n<p>유사하게 <code class=\"language-text\">SequenceFile.Reader</code>인스턴스를 사용해서 <code class=\"language-text\">SequenceFile</code>을 iteration을 통해 read할 수 있다.</p>\n<h3 id=\"mapfile\" style=\"position:relative;\"><a href=\"#mapfile\" aria-label=\"mapfile permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>MapFile</h3>\n<p><code class=\"language-text\">MapFile</code>은 key를 사용해서 lookup가능한 index로 정렬된 <code class=\"language-text\">SequenceFile</code>이다. index를 메모리에 저장해서 빠르게 검색한다는 장점이 있다. <code class=\"language-text\">set</code>, <code class=\"language-text\">array</code>, <code class=\"language-text\">bloommap</code>등의 자료구조를 포함한다. <code class=\"language-text\">set</code>과 <code class=\"language-text\">array</code>는 이미 익숙한 개념일거고, <code class=\"language-text\">bloommap</code>은 <code class=\"language-text\">get()</code> method를 조금 더 빠르게 처리해준다는 장점이 있는 자료구조이다. 자바스크립트에서 <code class=\"language-text\">Map</code>하고 비슷한 개념인 것 같다. 아??? 그래서 <code class=\"language-text\">MapFile</code>인가…</p>","fields":{"slug":"/posts/2022//hadoop/data-in-out","tagSlugs":["/tag/hadoop/"]},"frontmatter":{"date":"2022-10-19T23:35:37.121Z","description":"Hadoop이 데이터를 쓰고 읽는 법","tags":["Hadoop"],"title":"Hadoop I/O","socialImage":null}}},"pageContext":{"slug":"/posts/2022//hadoop/data-in-out"}},"staticQueryHashes":["251939775","288581551","401334301"]}